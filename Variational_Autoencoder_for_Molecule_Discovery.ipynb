{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9Gc1GfAAySwLi+U3PkLbC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrKenReid/VAE-for-Molecule-Discovery/blob/main/Variational_Autoencoder_for_Molecule_Discovery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder for Molecule Discovery\n",
        "\n",
        "This tutorial demonstrates how to use a Variational Autoencoder (VAE) for generating novel molecular structures. This approach is particularly useful in drug discovery, where we aim to generate new potential drug candidates.\n",
        "\n",
        "**Click runtime -> change runtime type -> GPU**\n",
        "\n",
        "1. Setting Up the Environment:\n",
        "We'll use PyTorch for deep learning and RDKit for cheminformatics. PyTorch is chosen for its flexibility and ease of use, while RDKit is an industry-standard tool for handling molecular data."
      ],
      "metadata": {
        "id": "7io8XFfVomej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_5846gQog8Z",
        "outputId": "871bb29c-0879-44d5-989e-d7fa01d9de25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: rdkit, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 rdkit-2024.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Preparation: We use the QM9 dataset, a standard benchmark in molecular machine learning. It contains about 134,000 small organic molecules."
      ],
      "metadata": {
        "id": "v8x5ac63eDJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_qm9_dataset(url: str = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download and load the QM9 dataset.\n",
        "\n",
        "    This function downloads the QM9 dataset from the specified URL using wget,\n",
        "    then loads it into a pandas DataFrame. It extracts the SMILES strings\n",
        "    from the DataFrame for further processing.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the QM9 dataset CSV file.\n",
        "                   Defaults to the DeepChem hosted QM9 dataset.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded QM9 dataset as a pandas DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        subprocess.CalledProcessError: If the wget command fails.\n",
        "        pd.errors.EmptyDataError: If the CSV file is empty or cannot be read.\n",
        "\n",
        "    Note:\n",
        "        This function requires wget to be installed and accessible in the system path.\n",
        "        It saves the downloaded file as 'qm9.csv' in the current directory.\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "\n",
        "    # Download the QM9 dataset\n",
        "    try:\n",
        "        subprocess.run(['wget', url], check=True)\n",
        "        print(f\"Dataset downloaded successfully from {url}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Failed to download dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Load the dataset\n",
        "    try:\n",
        "        df = pd.read_csv('qm9.csv')\n",
        "        print(f\"Loaded QM9 dataset with {len(df)} entries\")\n",
        "        return df\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"The CSV file is empty or could not be read\")\n",
        "        raise\n",
        "\n",
        "# go\n",
        "df = load_qm9_dataset()\n",
        "smiles_list = df['smiles'].tolist()\n",
        "print(f\"Extracted {len(smiles_list)} SMILES strings from the dataset\")"
      ],
      "metadata": {
        "id": "4GFZWKb_otA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9acf87-0f30-4541-f9dc-e6419d44d670"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded successfully from https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\n",
            "Loaded QM9 dataset with 133885 entries\n",
            "Extracted 133885 SMILES strings from the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Vocabulary: we create a vocabulary from the SMILES strings. This allows us to represent molecules as sequences of tokens that our model can process."
      ],
      "metadata": {
        "id": "EC-_jN2-iUFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def create_vocabulary(smiles_list: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Create a vocabulary mapping from SMILES characters to integer indices.\n",
        "\n",
        "    This function generates a vocabulary dictionary from a list of SMILES strings.\n",
        "    It includes special tokens for start ('<') and end ('>') of sequences, and\n",
        "    assigns a unique integer index to each unique character found in the SMILES strings.\n",
        "\n",
        "    Args:\n",
        "        smiles_list (List[str]): A list of SMILES strings to create the vocabulary from.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: A dictionary mapping characters to their corresponding integer indices.\n",
        "\n",
        "    Example:\n",
        "        >>> smiles_list = ['CC(=O)OC1=CC=CC=C1C(=O)O', 'C1=CC=C(C=C1)C(=O)O']\n",
        "        >>> vocab = create_vocabulary(smiles_list)\n",
        "        >>> print(vocab)\n",
        "        {'<': 0, '>': 1, 'C': 2, '(': 3, '=': 4, 'O': 5, ')': 6, '1': 7}\n",
        "\n",
        "    Note:\n",
        "        The returned vocabulary will always include '<' and '>' as the first two entries,\n",
        "        representing start and end tokens respectively.\n",
        "    \"\"\"\n",
        "    vocab = {'<': 0, '>': 1}  # Start and end tokens\n",
        "    for smiles in smiles_list:\n",
        "        for char in smiles:\n",
        "            if char not in vocab:\n",
        "                vocab[char] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "vocab = create_vocabulary(smiles_list)"
      ],
      "metadata": {
        "id": "p-7goMa8eF5H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dataset Creation: We create a custom dataset that converts SMILES strings into one-hot encoded tensors. This representation allows the model to process the molecular data efficiently."
      ],
      "metadata": {
        "id": "jbtTgxieiadd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SMILESDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset for SMILES strings.\n",
        "\n",
        "    This dataset converts SMILES strings into one-hot encoded tensors based on a given vocabulary.\n",
        "    It adds start ('<') and end ('>') tokens to each SMILES string before encoding.\n",
        "\n",
        "    Attributes:\n",
        "        smiles_list (List[str]): A list of SMILES strings.\n",
        "        vocab (Dict[str, int]): A dictionary mapping characters to their corresponding indices.\n",
        "        vocab_size (int): The size of the vocabulary.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, smiles_list: List[str], vocab: Dict[str, int], max_length: int = 100):\n",
        "        \"\"\"\n",
        "        Initialize the SMILESDataset.\n",
        "\n",
        "        Args:\n",
        "            smiles_list (List[str]): A list of SMILES strings to be encoded.\n",
        "            vocab (Dict[str, int]): A dictionary mapping characters to their corresponding indices.\n",
        "        \"\"\"\n",
        "        self.smiles_list = smiles_list\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of items in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of SMILES strings in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get a one-hot encoded tensor for a SMILES string at the given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the SMILES string to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A one-hot encoded tensor representing the SMILES string.\n",
        "                          Shape: (len(smiles), vocab_size)\n",
        "\n",
        "        Note:\n",
        "            Start ('<') and end ('>') tokens are added to the SMILES string before encoding.\n",
        "            If a character in the SMILES string is not in the vocabulary, it is encoded as the first token (index 0).\n",
        "        \"\"\"\n",
        "\n",
        "        smiles = '<' + self.smiles_list[idx] + '>'\n",
        "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
        "        for i, c in enumerate(smiles[:self.max_length]):\n",
        "            encoded[i, self.vocab.get(c, 0)] = 1\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "IMjVg9HaiWiu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model Architecture: We use a VAE architecture, which consists of an encoder, a latent space, and a decoder. This architecture is chosen because it can learn a compact representation of the input data and generate new samples from this representation."
      ],
      "metadata": {
        "id": "fjhCZwNeucTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Variational Autoencoder (VAE) for SMILES string generation.\n",
        "\n",
        "    This VAE encodes SMILES strings into a latent space and decodes latent\n",
        "    vectors back into SMILES strings. It uses GRU layers for both encoding and decoding.\n",
        "\n",
        "    Attributes:\n",
        "        vocab_size (int): Size of the SMILES vocabulary.\n",
        "        hidden_dim (int): Dimension of the hidden state in GRU layers.\n",
        "        latent_dim (int): Dimension of the latent space.\n",
        "        encoder (nn.GRU): GRU layer for encoding input sequences.\n",
        "        fc_mu (nn.Linear): Fully connected layer for mean of latent space.\n",
        "        fc_logvar (nn.Linear): Fully connected layer for log variance of latent space.\n",
        "        decoder (nn.GRU): GRU layer for decoding latent vectors.\n",
        "        fc_output (nn.Linear): Fully connected layer for output probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, hidden_dim: int, latent_dim: int):\n",
        "        \"\"\"\n",
        "        Initialize the VAE model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the SMILES vocabulary.\n",
        "            hidden_dim (int): Dimension of the hidden state in GRU layers.\n",
        "            latent_dim (int): Dimension of the latent space.\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.encoder = nn.GRU(vocab_size, hidden_dim, batch_first=True)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        self.decoder = nn.GRU(vocab_size + latent_dim, hidden_dim, batch_first=True)\n",
        "        self.fc_output = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Encode input sequences into the latent space.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, vocab_size).\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor]: Mean and log variance of the latent space distribution.\n",
        "        \"\"\"\n",
        "        _, h = self.encoder(x)\n",
        "        h = h.squeeze(0)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Perform the reparameterization trick to sample from the latent space.\n",
        "\n",
        "        Args:\n",
        "            mu (torch.Tensor): Mean of the latent space distribution.\n",
        "            logvar (torch.Tensor): Log variance of the latent space distribution.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Sampled latent vector.\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z: torch.Tensor, max_length: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Decode latent vectors into sequences.\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Latent vector of shape (batch_size, latent_dim).\n",
        "            max_length (int): Maximum length of the generated sequence.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Decoded sequences of shape (batch_size, max_length, vocab_size).\n",
        "        \"\"\"\n",
        "        batch_size = z.size(0)\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim).to(z.device)\n",
        "        x = torch.zeros(batch_size, 1, self.vocab_size).to(z.device)\n",
        "        outputs = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            z_input = z.unsqueeze(1).repeat(1, 1, 1)\n",
        "            decoder_input = torch.cat([x, z_input], dim=2)\n",
        "            output, h = self.decoder(decoder_input, h)\n",
        "            output = self.fc_output(output)\n",
        "            outputs.append(output)\n",
        "            x = torch.softmax(output, dim=-1)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through the VAE.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, vocab_size).\n",
        "\n",
        "        Returns:\n",
        "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "                - Reconstructed input of shape (batch_size, seq_len, vocab_size)\n",
        "                - Mean of the latent space distribution\n",
        "                - Log variance of the latent space distribution\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z, x.size(1)), mu, logvar"
      ],
      "metadata": {
        "id": "xQ7SipbQ2ITk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Training the Model: We train the model using a combination of reconstruction loss (binary [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy)) and [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)."
      ],
      "metadata": {
        "id": "OeCKitCCx4ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "def loss_function(recon_x: torch.Tensor, x: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "    BCE = nn.functional.binary_cross_entropy_with_logits(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + 0.1 * KLD\n",
        "\n",
        "def custom_collate(batch):\n",
        "    return torch.stack(batch)\n",
        "\n",
        "# Determine the number of workers based on system resources\n",
        "num_workers = min(2, os.cpu_count() or 1)\n",
        "\n",
        "# DataLoader setup\n",
        "batch_size = 128\n",
        "dataset = SMILESDataset(smiles_list, vocab, max_length=100)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate,\n",
        "                        num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "# Model initialization\n",
        "vocab_size = len(vocab)\n",
        "hidden_dim = 256\n",
        "latent_dim = 64\n",
        "\n",
        "model = VAE(vocab_size, hidden_dim, latent_dim)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimizer initialization\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "# Initialize mixed precision training if CUDA is available\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Use autocast only if CUDA is available\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            recon_batch, mu, logvar = model(batch)\n",
        "            loss = loss_function(recon_batch, batch, mu, logvar)\n",
        "\n",
        "        # Backward pass and optimization with gradient scaling if CUDA is available\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print epoch results\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    # Print current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Current learning rate: {current_lr:.6f}')\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btzV_OBbicNw",
        "outputId": "f44a815c-d4e8-4bce-ed4c-a11eeb52bedd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 97.0341\n",
            "Current learning rate: 0.001000\n",
            "Epoch 2, Average Loss: 48.1049\n",
            "Current learning rate: 0.001000\n",
            "Epoch 3, Average Loss: 42.9307\n",
            "Current learning rate: 0.001000\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Generating New Molecules: After training, we use the decoder to generate new molecular structures."
      ],
      "metadata": {
        "id": "NWg-IAEHyCjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_smiles(model: torch.nn.Module, vocab: Dict[str, int], num_samples: int = 10, max_length: int = 100) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate SMILES strings using the trained VAE model.\n",
        "\n",
        "    This function generates a specified number of SMILES strings by sampling from the\n",
        "    latent space and decoding the samples using the VAE's decoder.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained VAE model.\n",
        "        vocab (Dict[str, int]): The vocabulary mapping characters to indices.\n",
        "        num_samples (int, optional): Number of SMILES strings to generate. Defaults to 10.\n",
        "        max_length (int, optional): Maximum length of generated SMILES strings. Defaults to 100.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of generated SMILES strings.\n",
        "\n",
        "    Note:\n",
        "        - The function uses greedy decoding (taking the most probable character at each step).\n",
        "        - Generation stops when the end token '>' is produced or max_length is reached.\n",
        "        - Invalid characters (not in vocabulary) are ignored in the output.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    inv_vocab = {v: k for k, v in vocab.items()}  # Invert vocabulary for decoding\n",
        "    generated_smiles = []\n",
        "    device = next(model.parameters()).device  # Get the device of the model\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for _ in range(num_samples):\n",
        "            # Sample from the latent space\n",
        "            z = torch.randn(1, model.latent_dim).to(device)\n",
        "            x = torch.zeros(1, 1, model.vocab_size).to(device)\n",
        "            x[0, 0, vocab['<']] = 1  # Start token\n",
        "            h = torch.zeros(1, 1, model.hidden_dim).to(device)\n",
        "\n",
        "            smiles = ''\n",
        "            for _ in range(max_length):\n",
        "                z_input = z.unsqueeze(1)\n",
        "                decoder_input = torch.cat([x, z_input], dim=2)\n",
        "                output, h = model.decoder(decoder_input, h)\n",
        "                output = model.fc_output(output)\n",
        "\n",
        "                probs = torch.softmax(output.squeeze(0), dim=-1)\n",
        "                next_char = torch.multinomial(probs, 1).item()\n",
        "\n",
        "                if next_char == vocab['>']:  # End token\n",
        "                    break\n",
        "\n",
        "                smiles += inv_vocab.get(next_char, '')\n",
        "                x = torch.zeros(1, 1, model.vocab_size).to(device)\n",
        "                x[0, 0, next_char] = 1\n",
        "\n",
        "            generated_smiles.append(smiles)\n",
        "\n",
        "    return generated_smiles\n",
        "\n",
        "# Generate and print some SMILES strings\n",
        "num_samples = 10\n",
        "generated_smiles = generate_smiles(model, vocab, num_samples=num_samples)\n",
        "print(f\"Generated {num_samples} SMILES strings:\")\n",
        "for i, smiles in enumerate(generated_smiles, 1):\n",
        "    print(f\"{i}. {smiles}\")"
      ],
      "metadata": {
        "id": "Fv1slHmsx6lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1321e9-7fe8-4e11-922e-23bd850a7c7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 10 SMILES strings:\n",
            "1. <C1CO1C11#1\n",
            "2. <CO1CCCCCC=1O1\n",
            "3. FO54\n",
            "4. \n",
            "5. <CC111CC2COCCCC2C\n",
            "6. +=N(CC32CCCC1\n",
            "7. )N11=24CC12C2\n",
            "8. ]N4+F\n",
            "9. 3(C#=NCCC1CCCC\n",
            "10. <C1CCC2((C)C1==C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Analyzing Generated Molecules: After generating new molecules, we analyze them for validity, novelty, and potential usefulness. Here are some common analyses and insights we can derive:\n",
        "\n",
        "  7.1 Validity Check: First, we'll check how many of our generated SMILES strings represent valid molecules:"
      ],
      "metadata": {
        "id": "ObYXPrUWzboS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "def validate_and_correct_smiles(smiles: str) -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Validate and attempt to correct a SMILES string.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): The SMILES string to validate and correct.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, str]: A tuple containing a boolean indicating validity\n",
        "                          and the corrected SMILES string (or original if invalid).\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        try:\n",
        "            Chem.SanitizeMol(mol)\n",
        "            return True, Chem.MolToSmiles(mol, isomericSmiles=True)\n",
        "        except:\n",
        "            pass\n",
        "    return False, smiles\n",
        "\n",
        "def enhanced_post_process_smiles(smiles: str) -> str:\n",
        "    \"\"\"\n",
        "    Post-process a SMILES string to correct common issues.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): The SMILES string to process.\n",
        "\n",
        "    Returns:\n",
        "        str: The processed SMILES string.\n",
        "    \"\"\"\n",
        "    smiles = smiles.replace('<', '').replace('>', '')\n",
        "    allowed_chars = set('CNOPSFIBrClcnops()[]=@+-#0123456789')\n",
        "    smiles = ''.join(c for c in smiles if c in allowed_chars)\n",
        "\n",
        "    # Balance parentheses\n",
        "    open_count = smiles.count('(')\n",
        "    close_count = smiles.count(')')\n",
        "    if open_count > close_count:\n",
        "        smiles += ')' * (open_count - close_count)\n",
        "    elif close_count > open_count:\n",
        "        smiles = '(' * (close_count - open_count) + smiles\n",
        "\n",
        "    # Replace invalid double bonds\n",
        "    smiles = smiles.replace('==', '=')\n",
        "\n",
        "    # Attempt to close unclosed rings\n",
        "    for i in range(1, 10):\n",
        "        if smiles.count(str(i)) % 2 != 0:\n",
        "            smiles += str(i)\n",
        "\n",
        "    return smiles\n",
        "\n",
        "def analyze_molecules(smiles_list: List[str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Analyze a list of SMILES strings and provide detailed statistics.\n",
        "\n",
        "    Args:\n",
        "        smiles_list (List[str]): A list of SMILES strings to analyze.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary containing various statistics and analyses.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'total': len(smiles_list),\n",
        "        'valid': 0,\n",
        "        'invalid': 0,\n",
        "        'unique': 0,\n",
        "        'corrected': 0,\n",
        "        'properties': [],\n",
        "        'invalid_smiles': []\n",
        "    }\n",
        "\n",
        "    unique_smiles = set()\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        processed_smiles = enhanced_post_process_smiles(smiles)\n",
        "        is_valid, corrected_smiles = validate_and_correct_smiles(processed_smiles)\n",
        "\n",
        "        if is_valid:\n",
        "            results['valid'] += 1\n",
        "            unique_smiles.add(corrected_smiles)\n",
        "            if corrected_smiles != processed_smiles:\n",
        "                results['corrected'] += 1\n",
        "\n",
        "            mol = Chem.MolFromSmiles(corrected_smiles)\n",
        "            results['properties'].append({\n",
        "                'smiles': corrected_smiles,\n",
        "                'molecular_weight': Descriptors.ExactMolWt(mol),\n",
        "                'logp': Descriptors.MolLogP(mol),\n",
        "                'num_atoms': mol.GetNumAtoms(),\n",
        "                'num_rings': Descriptors.RingCount(mol)\n",
        "            })\n",
        "        else:\n",
        "            results['invalid'] += 1\n",
        "            results['invalid_smiles'].append(smiles)\n",
        "\n",
        "    results['unique'] = len(unique_smiles)\n",
        "    return results\n",
        "\n",
        "# Assume generated_smiles is your list of generated SMILES strings\n",
        "analysis = analyze_molecules(generated_smiles)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Total SMILES: {analysis['total']}\")\n",
        "print(f\"Valid SMILES: {analysis['valid']} ({analysis['valid']/analysis['total']:.2%})\")\n",
        "print(f\"Invalid SMILES: {analysis['invalid']} ({analysis['invalid']/analysis['total']:.2%})\")\n",
        "print(f\"Unique SMILES: {analysis['unique']} ({analysis['unique']/analysis['total']:.2%})\")\n",
        "print(f\"Corrected SMILES: {analysis['corrected']} ({analysis['corrected']/analysis['total']:.2%})\")\n",
        "\n",
        "# Print property statistics\n",
        "df = pd.DataFrame(analysis['properties'])\n",
        "print(\"\\nProperty Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Print some invalid SMILES\n",
        "print(\"\\nSample of Invalid SMILES:\")\n",
        "for smiles in analysis['invalid_smiles'][:5]:  # Print first 5 invalid SMILES\n",
        "    print(smiles)\n",
        "\n",
        "# Optionally, save the full analysis to a CSV file\n",
        "df.to_csv('molecule_properties.csv', index=False)\n",
        "print(\"\\nFull property analysis saved to 'molecule_properties.csv'\")"
      ],
      "metadata": {
        "id": "p1Lj92Eezp6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62348ac3-5d76-4ddd-d118-1098a6cf2478"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total SMILES: 10\n",
            "Valid SMILES: 1 (10.00%)\n",
            "Invalid SMILES: 9 (90.00%)\n",
            "Unique SMILES: 1 (10.00%)\n",
            "Corrected SMILES: 0 (0.00%)\n",
            "\n",
            "Property Statistics:\n",
            "       molecular_weight  logp  num_atoms  num_rings\n",
            "count               1.0   1.0        1.0        1.0\n",
            "mean                0.0   0.0        0.0        0.0\n",
            "std                 NaN   NaN        NaN        NaN\n",
            "min                 0.0   0.0        0.0        0.0\n",
            "25%                 0.0   0.0        0.0        0.0\n",
            "50%                 0.0   0.0        0.0        0.0\n",
            "75%                 0.0   0.0        0.0        0.0\n",
            "max                 0.0   0.0        0.0        0.0\n",
            "\n",
            "Sample of Invalid SMILES:\n",
            "<C1CO1C11#1\n",
            "<CO1CCCCCC=1O1\n",
            "FO54\n",
            "<CC111CC2COCCCC2C\n",
            "+=N(CC32CCCC1\n",
            "\n",
            "Full property analysis saved to 'molecule_properties.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[14:02:32] SMILES Parse Error: duplicated ring closure 1 bonds atom 3 to itself for input: 'C1CO1C11#11'\n",
            "[14:02:32] SMILES Parse Error: duplicated ring closure 1 bonds atom 8 to itself for input: 'CO1CCCCCC=1O11'\n",
            "[14:02:32] SMILES Parse Error: duplicated ring closure 4 bonds atom 1 to itself for input: 'FO5445'\n",
            "[14:02:32] SMILES Parse Error: duplicated ring closure 1 bonds atom 1 to itself for input: 'CC111CC2COCCCC2C1'\n",
            "[14:02:32] SMILES Parse Error: syntax error while parsing: +=N(CC32CCCC1)123\n",
            "[14:02:32] SMILES Parse Error: Failed parsing SMILES '+=N(CC32CCCC1)123' for input: '+=N(CC32CCCC1)123'\n",
            "[14:02:32] SMILES Parse Error: syntax error while parsing: ()N11=24CC12C2124\n",
            "[14:02:32] SMILES Parse Error: Failed parsing SMILES '()N11=24CC12C2124' for input: '()N11=24CC12C2124'\n",
            "[14:02:32] SMILES Parse Error: syntax error while parsing: ]N4+F4\n",
            "[14:02:32] SMILES Parse Error: Failed parsing SMILES ']N4+F4' for input: ']N4+F4'\n",
            "[14:02:32] SMILES Parse Error: syntax error while parsing: 3(C#=NCCC1CCCC)13\n",
            "[14:02:32] SMILES Parse Error: Failed parsing SMILES '3(C#=NCCC1CCCC)13' for input: '3(C#=NCCC1CCCC)13'\n",
            "[14:02:32] SMILES Parse Error: syntax error while parsing: C1CCC2((C)C1=C)2\n",
            "[14:02:32] SMILES Parse Error: Failed parsing SMILES 'C1CCC2((C)C1=C)2' for input: 'C1CCC2((C)C1=C)2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.2 Novelty Assessment\n",
        "We can check if our generated molecules are novel (not present in the training set):"
      ],
      "metadata": {
        "id": "7oZQYmBv4uuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Set\n",
        "\n",
        "def check_novelty(generated_smiles: List[str], training_smiles: Set[str]) -> tuple[List[str], float]:\n",
        "    \"\"\"\n",
        "    Check the novelty of generated SMILES strings against a training set.\n",
        "\n",
        "    This function determines which of the generated SMILES strings are not present\n",
        "    in the training set, and calculates the novelty rate.\n",
        "\n",
        "    Args:\n",
        "        generated_smiles (List[str]): A list of SMILES strings generated by the model.\n",
        "        training_smiles (Set[str]): A set of SMILES strings used for training the model.\n",
        "\n",
        "    Returns:\n",
        "        tuple[List[str], float]: A tuple containing:\n",
        "            - A list of novel SMILES strings (those not in the training set).\n",
        "            - The novelty rate as a float (proportion of novel SMILES).\n",
        "\n",
        "    Example:\n",
        "        >>> training_set = {\"CC\", \"CCO\", \"CCCC\"}\n",
        "        >>> generated_smiles = [\"CC\", \"CCC\", \"CCCO\", \"CCO\"]\n",
        "        >>> novel, rate = check_novelty(generated_smiles, training_set)\n",
        "        >>> print(novel)\n",
        "        ['CCC', 'CCCO']\n",
        "        >>> print(f\"Novelty rate: {rate:.2%}\")\n",
        "        Novelty rate: 50.00%\n",
        "    \"\"\"\n",
        "    novel_molecules = [s for s in generated_smiles if s not in training_smiles]\n",
        "    novelty_rate = len(novel_molecules) / len(generated_smiles)\n",
        "    return novel_molecules, novelty_rate\n",
        "\n",
        "training_set = set(smiles_list)  # Convert training list to a set for faster lookup\n",
        "novel_molecules, novelty_rate = check_novelty(generated_smiles, training_set)\n",
        "\n",
        "print(f\"Novelty rate: {novelty_rate:.2%}\")\n",
        "print(f\"Number of novel molecules: {len(novel_molecules)}\")\n",
        "print(\"\\nSample of novel molecules:\")\n",
        "for smiles in novel_molecules[:5]:  # Print first 5 novel SMILES\n",
        "    print(smiles)"
      ],
      "metadata": {
        "id": "YZDVwniQ4w1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e579bf2b-b423-40e9-b7e8-c7c52efbb3cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Novelty rate: 100.00%\n",
            "Number of novel molecules: 10\n",
            "\n",
            "Sample of novel molecules:\n",
            "<C1CO1C11#1\n",
            "<CO1CCCCCC=1O1\n",
            "FO54\n",
            "\n",
            "<CC111CC2COCCCC2C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.3 Basic Property Calculation: We can calculate some basic molecular properties for valid molecules:"
      ],
      "metadata": {
        "id": "tbV5wLh54z6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "def calculate_properties(smiles: str) -> Optional[Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Calculate molecular properties for a given SMILES string.\n",
        "\n",
        "    This function computes several key molecular properties using RDKit's Descriptors module.\n",
        "    If the SMILES string is invalid or cannot be converted to a molecule, the function returns None.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): A SMILES string representing a molecule.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, float]]: A dictionary containing the calculated properties, or None if the SMILES is invalid.\n",
        "        The dictionary includes the following properties:\n",
        "            - 'MolWt': Exact molecular weight\n",
        "            - 'LogP': Octanol-water partition coefficient\n",
        "            - 'NumHDonors': Number of hydrogen bond donors\n",
        "            - 'NumHAcceptors': Number of hydrogen bond acceptors\n",
        "\n",
        "    Example:\n",
        "        >>> props = calculate_properties(\"CCO\")\n",
        "        >>> print(props)\n",
        "        {'MolWt': 46.06844, 'LogP': -0.0014, 'NumHDonors': 1, 'NumHAcceptors': 1}\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        return {\n",
        "            'MolWt': Descriptors.ExactMolWt(mol),\n",
        "            'LogP': Descriptors.MolLogP(mol),\n",
        "            'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "            'NumHAcceptors': Descriptors.NumHAcceptors(mol)\n",
        "        }\n",
        "    return None\n",
        "\n",
        "def calculate_properties_for_valid_mols(generated_smiles: List[str]) -> List[Optional[Dict[str, float]]]:\n",
        "    \"\"\"\n",
        "    Calculate properties for a list of generated SMILES strings, considering only valid molecules.\n",
        "\n",
        "    This function iterates through the list of generated SMILES strings, calculates properties\n",
        "    for valid molecules, and returns a list of property dictionaries.\n",
        "\n",
        "    Args:\n",
        "        generated_smiles (List[str]): A list of SMILES strings generated by the model.\n",
        "\n",
        "    Returns:\n",
        "        List[Optional[Dict[str, float]]]: A list of dictionaries containing calculated properties\n",
        "        for valid molecules. Invalid SMILES strings are excluded from the result.\n",
        "\n",
        "    Example:\n",
        "        >>> smiles_list = [\"CCO\", \"InvalidSMILES\", \"C1=CC=CC=C1\"]\n",
        "        >>> properties_list = calculate_properties_for_valid_mols(smiles_list)\n",
        "        >>> print(len(properties_list))\n",
        "        2\n",
        "    \"\"\"\n",
        "    return [calculate_properties(s) for s in generated_smiles if Chem.MolFromSmiles(s)]\n",
        "\n",
        "valid_mols_properties = calculate_properties_for_valid_mols(smiles)\n",
        "\n",
        "print(f\"Number of valid molecules: {len(valid_mols_properties)}\")\n",
        "for i, props in enumerate(valid_mols_properties, 1):\n",
        "    if props:\n",
        "        print(f\"Molecule {i}:\")\n",
        "        for key, value in props.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "8RZ6qW3B428z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbbe1ec-118e-4010-a4b1-64f45ff70cf3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of valid molecules: 11\n",
            "Molecule 1:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 2:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 3:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 4:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 5:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 6:\n",
            "  MolWt: 18.010564684\n",
            "  LogP: -0.8247\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 7:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 8:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 9:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 10:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n",
            "Molecule 11:\n",
            "  MolWt: 16.031300127999998\n",
            "  LogP: 0.6361\n",
            "  NumHDonors: 0\n",
            "  NumHAcceptors: 0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[14:02:37] SMILES Parse Error: syntax error while parsing: <\n",
            "[14:02:37] SMILES Parse Error: Failed parsing SMILES '<' for input: '<'\n",
            "[14:02:37] SMILES Parse Error: syntax error while parsing: 1\n",
            "[14:02:37] SMILES Parse Error: Failed parsing SMILES '1' for input: '1'\n",
            "[14:02:37] SMILES Parse Error: syntax error while parsing: 1\n",
            "[14:02:37] SMILES Parse Error: Failed parsing SMILES '1' for input: '1'\n",
            "[14:02:37] SMILES Parse Error: syntax error while parsing: 1\n",
            "[14:02:37] SMILES Parse Error: Failed parsing SMILES '1' for input: '1'\n",
            "[14:02:37] SMILES Parse Error: syntax error while parsing: 2\n",
            "[14:02:37] SMILES Parse Error: Failed parsing SMILES '2' for input: '2'\n",
            "[14:02:37] SMILES Parse Error: syntax error while parsing: 2\n",
            "[14:02:37] SMILES Parse Error: Failed parsing SMILES '2' for input: '2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.4 Visualizing Generated Molecules: We can visualize some of the valid generated molecules:"
      ],
      "metadata": {
        "id": "tHEYVIQQ4668"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from typing import List, Optional, Tuple\n",
        "import logging\n",
        "from PIL import Image\n",
        "import io\n",
        "from IPython.display import display\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def visualize_molecules(smiles_list: List[str], n: int = 5, filename: str = \"generated_molecules.png\") -> Optional[Tuple[str, Image.Image]]:\n",
        "    \"\"\"\n",
        "    Visualize a list of generated molecules from their SMILES strings, save and return the image.\n",
        "\n",
        "    This function takes the list of generated SMILES strings, converts them to RDKit molecule objects,\n",
        "    generates a grid image of the molecules, and both saves and returns the image.\n",
        "\n",
        "    Args:\n",
        "        smiles_list (List[str]): The list of generated SMILES strings to visualize.\n",
        "        n (int, optional): Maximum number of molecules to visualize. Defaults to 5.\n",
        "        filename (str, optional): Name of the file to save the image. Defaults to \"generated_molecules.png\".\n",
        "\n",
        "    Returns:\n",
        "        Optional[Tuple[str, Image.Image]]: A tuple containing the filename of the saved image and the Image object if successful, None otherwise.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If no valid molecules are found in the input list.\n",
        "    \"\"\"\n",
        "    valid_mols = []\n",
        "    invalid_count = 0\n",
        "    for i, smiles in enumerate(smiles_list):\n",
        "        # Remove any start/end tokens and whitespace\n",
        "        smiles = smiles.strip().strip('<>').strip()\n",
        "        if not smiles:  # Skip empty strings\n",
        "            invalid_count += 1\n",
        "            continue\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is not None:\n",
        "                valid_mols.append(mol)\n",
        "                if len(valid_mols) == n:\n",
        "                    break\n",
        "            else:\n",
        "                logger.warning(f\"Invalid SMILES at index {i}: {smiles}\")\n",
        "                invalid_count += 1\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing SMILES at index {i}: {smiles}. Error: {str(e)}\")\n",
        "            invalid_count += 1\n",
        "\n",
        "    if not valid_mols:\n",
        "        logger.error(\"No valid molecules found in the generated SMILES list.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"{invalid_count} molecules not drawn due to invalidity. Drawing {len(valid_mols)} molecules...\")\n",
        "\n",
        "    try:\n",
        "        img = Draw.MolsToGridImage(\n",
        "            valid_mols,\n",
        "            molsPerRow=min(3, len(valid_mols)),\n",
        "            subImgSize=(200, 200),\n",
        "            legends=[f\"Mol {i+1}\" for i in range(len(valid_mols))]\n",
        "        )\n",
        "        # Convert the RDKit image to a PIL Image\n",
        "        img_pil = Image.open(io.BytesIO(img.data))\n",
        "        img_pil.save(filename)\n",
        "        logger.info(f\"Generated molecule images saved as '{filename}'\")\n",
        "        return filename, img_pil\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving generated molecule image: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# The function call and display code\n",
        "result = visualize_molecules(smiles)\n",
        "\n",
        "if result:\n",
        "    filename, img = result\n",
        "    print(f\"Visualization of generated molecules successful. Image saved as: {filename}\")\n",
        "    display(img)\n",
        "else:\n",
        "    print(\"Visualization of generated molecules failed.\")"
      ],
      "metadata": {
        "id": "96u_znL3488U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "0d396e1e-68f9-46dc-983d-c9a8b7005278"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[14:16:39] SMILES Parse Error: syntax error while parsing: 1\n",
            "[14:16:39] SMILES Parse Error: Failed parsing SMILES '1' for input: '1'\n",
            "WARNING:__main__:Invalid SMILES at index 3: 1\n",
            "[14:16:39] SMILES Parse Error: syntax error while parsing: 1\n",
            "[14:16:39] SMILES Parse Error: Failed parsing SMILES '1' for input: '1'\n",
            "WARNING:__main__:Invalid SMILES at index 4: 1\n",
            "[14:16:39] SMILES Parse Error: syntax error while parsing: 1\n",
            "[14:16:39] SMILES Parse Error: Failed parsing SMILES '1' for input: '1'\n",
            "WARNING:__main__:Invalid SMILES at index 5: 1\n",
            "[14:16:39] SMILES Parse Error: syntax error while parsing: 2\n",
            "[14:16:39] SMILES Parse Error: Failed parsing SMILES '2' for input: '2'\n",
            "WARNING:__main__:Invalid SMILES at index 8: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 molecules not drawn due to invalidity. Drawing 5 molecules...\n",
            "Visualization of generated molecules successful. Image saved as: generated_molecules.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=600x400>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.5 Insights and Discussion: After performing these analyses, we can discuss the results:\n",
        "\n",
        "* Validity Rate: A high validity rate indicates that our model has learned to\n",
        " generate syntactically correct SMILES strings. However, if it's too high (close to 100%), it might suggest that the model is playing it safe and not exploring the chemical space enough.\n",
        "* Novelty Rate: A high novelty rate suggests that our model is not just memorizing the training set but generating new structures. This is crucial for drug discovery applications.\n",
        "* Property Distribution: By analyzing the distribution of molecular properties (like molecular weight, LogP, etc.), we can see if our generated molecules are similar to the training set or if they're exploring new areas of chemical space.\n",
        "* Visual Inspection: Visualizing the molecules allows for a qualitative assessment of the types of structures being generated. This can help identify any patterns or biases in the generation process.\n",
        "* Comparison to Training Set: We can compare the properties of generated molecules to those in the training set to see how well our model has captured the distribution of the original data.\n",
        "\n",
        "These analyses provide insights into the performance of our VAE model and the characteristics of the generated molecules. They can guide further refinement of the model or selection of promising candidates for more detailed investigation in a drug discovery pipeline."
      ],
      "metadata": {
        "id": "sUOfwQRS5DmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "This tutorial demonstrates how to use a VAE for molecular generation, combining deep learning techniques with chemical knowledge to create a tool for drug discovery. By understanding and modifying this code, biomedical researchers can adapt it to their specific research needs, potentially accelerating the drug discovery process."
      ],
      "metadata": {
        "id": "69Ki1O9dyHxi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRg0BRxZ49Q-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}